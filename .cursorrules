# BrainDock - Cursor Rules

## Project Overview

BrainDock is an AI-powered study focus tracking application that monitors students via webcam, detects distractions (gadgets like phones, tablets, game controllers, etc.), and generates detailed PDF reports with AI-generated insights.

**Tech Stack:**
- Python 3.9+
- tkinter for desktop GUI
- OpenAI Vision API (gpt-4o-mini) for detection
- OpenAI GPT API for summaries
- OpenCV for camera capture
- ReportLab for PDF generation

**Key Architecture:**
- Real-time detection using OpenAI Vision API (1 FPS)
- Session-based tracking with JSON persistence
- Event logging (present, away, gadget_suspected)
- AI-generated insights and suggestions

## Code Standards

### General
- Use descriptive variable, function, and class names
- Add simple, understandable comments where relevant (don't overdo it)
- Add docstrings for every function
- Keep code human-like and readable
- Well-organized and modular with clear separation of concerns
- Use type hints for function parameters and returns

### Python Specific
- Python 3.9+ features
- Use pathlib for file paths
- Use type hints (from typing import ...)
- Use dataclasses where appropriate
- Follow PEP 8 style guide
- Use logging module (not print) for internal logs

## Project Structure

```
braindock/
├── main.py                    # Main entry point (GUI default, --cli for CLI)
├── config.py                  # All configuration constants
├── gui/
│   ├── __init__.py
│   ├── app.py                # Desktop GUI application (tkinter)
│   ├── payment_screen.py     # License/payment screen
│   └── ui_components.py      # Reusable UI components
├── camera/
│   ├── capture.py            # Webcam management (OpenCV)
│   └── vision_detector.py    # AI-powered detection (OpenAI Vision API)
├── tracking/
│   ├── session.py            # Session class with event logging
│   ├── analytics.py          # Statistics computation
│   └── usage_limiter.py      # MVP usage time tracking
├── screen/
│   ├── window_detector.py    # Screen/window monitoring
│   └── blocklist.py          # Distraction blocklist management
├── licensing/
│   ├── license_manager.py    # License validation
│   └── stripe_integration.py # Payment processing
├── reporting/
│   └── pdf_report.py         # PDF generation (ReportLab)
├── data/sessions/            # JSON session files
└── tests/                    # Unit tests

Reports are saved to: ~/Downloads/
```

## Key Architectural Decisions

### 1. AI-Powered Detection (Primary System)
- **ALL detection uses OpenAI Vision API** (no hardcoded methods)
- Send camera frames to OpenAI every second (configurable via DETECTION_FPS)
- Vision API returns: person_present, at_desk, gadget_visible (ACTIVE usage only), distraction_type
- **Distance-aware presence detection**:
  - `person_present`: True if any body part visible (even far away)
  - `at_desk`: True only if person is at working distance (face/upper body clearly visible)
  - Student roaming 10m away = person_present=true, at_desk=false = marked as "away"
- **Gadget detection is smart**: Detects phones, tablets, game controllers, Nintendo Switch, TV, etc.
  - Requires: Person looking at gadget AND actively engaged with it
  - Position irrelevant: Gadget can be on desk or in hands
- **Prevents false positives**: Gadget on desk with person looking elsewhere = NOT detected
- NO fallback methods - AI is the only detection method

### 2. Privacy & Data
- We capture frames for analysis; we don't store them locally
- OpenAI retains up to 30 days for safety monitoring per [API policy](https://openai.com/policies/api-data-usage-policies)
- Session data saved as JSON (timestamps + event types only)
- Frames never saved to disk

### 3. Event System
Three event types tracked:
- `present`: Person at desk (close to camera), focussed
- `away`: Person not visible OR visible but far from desk (roaming around room)
- `gadget_suspected`: Gadget distraction detected (phone, tablet, controller, etc.)

Events have: type, start time, end time, duration

### 4. Cost Optimisation
- Vision API is expensive (~$0.06-0.12 per minute)
- Use caching (1 second cache duration)
- Use "low" detail in vision requests
- Configurable detection frequency (DETECTION_FPS)

## Important Constants (config.py)

```python
# AI Models
OPENAI_MODEL = "gpt-4o-mini"  # Text summaries
OPENAI_VISION_MODEL = "gpt-4o-mini"  # Image analysis

# Detection settings
DETECTION_FPS = 1  # Frames per second to analyse
GADGET_CONFIDENCE_THRESHOLD = 0.5  # Vision API confidence
GADGET_DETECTION_DURATION_SECONDS = 2  # Sustained detection

# Paths
DATA_DIR = BASE_DIR / "data" / "sessions"
REPORTS_DIR = BASE_DIR / "reports"
```

## Module Guidelines

### gui/app.py
- Main desktop GUI application using tkinter
- BrainDockGUI class manages the application window
- Features: Start/Stop button, status indicator, timer, report generation
- Runs detection in separate thread for responsive UI
- Uses thread-safe UI updates via root.after()
- Modern dark theme with teal/coral accents
- Privacy notice popup on first launch
- **Blocklist validation**: `KNOWN_APPS` (1500+ apps), `VALID_TLDS` - DO NOT load full lists into context
  - URL validation: TLD + DNS lookup with graceful network fallback
  - App validation: Whitelist check, warns about unknown apps

### camera/vision_detector.py
- Primary detection system
- Uses OpenAI Vision API
- `analyze_frame()` is the main entry point
- Returns structured detection dictionary
- Implements caching to reduce API calls
- Handles JSON parsing from Vision API responses

### tracking/session.py
- Session lifecycle management
- Event logging with state change detection
- JSON serialization/deserialization
- Print console updates for major state changes

### tracking/analytics.py
- Compute statistics from events
- Math MUST add up: present + away + gadget = total
- Event consolidation (merge consecutive similar events)
- Time formatting helpers
- generate_summary_text() provides fallback summary if needed

### reporting/pdf_report.py
- Professional PDF generation using ReportLab
- format_duration() from tracking/analytics.py for human-readable durations (1m 30s not 1.5 minutes)
- Statistics table, timeline, AI insights
- Color-coded, well-structured layout

## Common Patterns

### Error Handling
```python
try:
    # Operation
    result = operation()
except Exception as e:
    logger.error(f"Error in operation: {e}")
    # Return safe default or re-raise
```

### API Calls with Retry
```python
for attempt in range(max_retries):
    try:
        response = api_call()
        return response
    except Exception as e:
        if attempt < max_retries - 1:
            time.sleep(retry_delay * (2 ** attempt))
        else:
            raise
```

### Time Formatting
```python
# Always use format_duration() for display
# Returns: "45s", "1m 30s", "2h 15m"
from tracking.analytics import format_duration
display_time = format_duration(seconds)
```

## Important Constraints

### What NOT to Do
- ❌ NO hardcoded detection methods (no MediaPipe, no OpenCV shapes)
- ❌ NO fallback detection (AI-only by design)
- ❌ NO saving video frames to disk
- ❌ NO generic AI cheerleading ("Great job! Amazing!")
- ❌ NO statistics that don't add up
- ❌ NO time displays like "0.0 minutes"

### What TO Do
- ✅ Use OpenAI Vision API for all detection
- ✅ Direct, factual AI tone
- ✅ Ensure math always adds up (present + away + gadget = total)
- ✅ Format times as "1m 30s" not "1.5 minutes"
- ✅ Log important events to logger
- ✅ Add docstrings to all functions
- ✅ Handle API errors gracefully

## Testing

### Unit Tests
- Session tracking: tests/test_session.py
- Analytics: tests/test_analytics.py
- PDF generation: tests/test_pdf_report.py
- Run with: `python3 -m unittest tests.test_session`

### Integration Tests
- test_gadget_detection.py: Interactive Vision API testing with camera
- Manual testing: camera detection, PDF generation

### Manual Testing Checklist
1. GUI launches successfully
2. Privacy notice appears on first launch
3. Camera opens successfully
4. Vision API detects person presence
5. Vision API detects gadgets when in use
6. Status indicator updates in real-time
7. Timer displays correctly
8. Events logged correctly
9. Statistics math adds up
10. PDF generates successfully
11. AI summary is direct and specific

## Cost Awareness

**Always consider API costs when making changes:**
- Each vision API call: ~$0.001-0.002
- 1 FPS = 60 calls/minute = ~$0.06-0.12/min
- 2 FPS doubles the cost
- Balance accuracy vs cost

## Future Extensibility

### Adding New Detection Types
1. Update vision_detector.py prompt to include new field
2. Add new event type to config.py
3. Update session.py to handle new event type
4. Update analytics to compute stats for new type
5. Update PDF to display new metric

### Adding New AI Models
- All models configured in config.py
- System auto-detects JSON mode support
- Easy to swap models (just change constant)

## Dependencies Management

### Core Dependencies
```
opencv-python>=4.8.0      # Camera capture
openai>=1.0.0             # AI integration
reportlab>=4.0.0          # PDF generation
python-dotenv>=1.0.0      # Environment variables
```

### Company Network Considerations
- Uses Square's Artifactory mirror for packages
- May require internal network for installations
- No external model downloads (all via OpenAI API)

## Development Workflow

1. **Make changes** to appropriate module
2. **Test locally** with short sessions
3. **Check OpenAI usage** on dashboard
4. **Verify PDF output** looks correct
5. **Check logs** for any errors
6. **Commit** with descriptive messages

## Common Issues & Solutions

### Issue: "Vision API Error: Expecting value"
**Solution:** Check JSON parsing in vision_detector.py, handle markdown code blocks

### Issue: "Statistics don't add up"
**Solution:** Verify analytics.py: present + away + gadget should equal total

### Issue: "Gadget not detected"
**Solution:** Check Vision API logs, verify confidence threshold, ensure gadget visible and in active use

### Issue: "API key not found"
**Solution:** Verify .env file exists with OPENAI_API_KEY=sk-...

### Issue: "Credits not decreasing"
**Solution:** Check if Vision API is actually being called (look for HTTP POST logs)

## When Helping With This Project

1. **Understand the AI-first architecture** - Everything uses OpenAI now
2. **Be cost-aware** - Vision API is expensive, optimise when possible
3. **Maintain code quality** - Docstrings, type hints, clean code
4. **Test thoroughly** - Changes affect real-time detection
5. **Document changes** - Update relevant .md files
6. **Consider privacy** - Be transparent about what's sent to OpenAI

## Current State

**Version:** 1.1 (GUI Edition)
**Status:** Fully functional with GUI
**Architecture:** AI-first (OpenAI Vision + GPT)
**Detection:** Working via Vision API
**Interface:** Desktop GUI (tkinter) with CLI fallback
**Known Issues:** None critical
**Next Steps:** macOS/Windows packaging, dashboard, better PDF visualizations
